\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{parkhi_cats_2012}
\citation{huang_speed/accuracy_2016}
\citation{martin_abadi_tensorflow:_2015}
\citation{huang_speed/accuracy_2016}
\citation{huang_speed/accuracy_2016}
\citation{nash_topological_2015}
\citation{huang_speed/accuracy_2016}
\citation{nash_topological_2015}
\citation{huang_speed/accuracy_2016}
\citation{nash_topological_2015}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Real-world object detection using bounding boxes, from \cite  {huang_speed/accuracy_2016}. Class labels and confidence scores accompany each box. Objects detected with $> 50\%$ confidence shown.\relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:real-world}{{1}{1}{Real-world object detection using bounding boxes, from \cite {huang_speed/accuracy_2016}. Class labels and confidence scores accompany each box. Objects detected with $> 50\%$ confidence shown.\relax }{figure.caption.1}{}}
\citation{huang_speed/accuracy_2016}
\citation{krizhevsky_imagenet_2012}
\citation{huang_speed/accuracy_2016}
\newlabel{fig:bare-gyros}{{2a}{2}{\relax }{figure.caption.2}{}}
\newlabel{sub@fig:bare-gyros}{{a}{2}{\relax }{figure.caption.2}{}}
\newlabel{fig:single-bare-gyro}{{2b}{2}{\relax }{figure.caption.2}{}}
\newlabel{sub@fig:single-bare-gyro}{{b}{2}{\relax }{figure.caption.2}{}}
\newlabel{fig:bounding-boxes}{{2c}{2}{\relax }{figure.caption.2}{}}
\newlabel{sub@fig:bounding-boxes}{{c}{2}{\relax }{figure.caption.2}{}}
\newlabel{fig:bounding-boxes-shifted}{{2d}{2}{\relax }{figure.caption.2}{}}
\newlabel{sub@fig:bounding-boxes-shifted}{{d}{2}{\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces In (\textbf  {a}), the full gyroscopic model for topological metamaterials from \cite  {nash_topological_2015}, where each individual gyro (\textbf  {b}) is constrained to motion inside its circle. The center ``dot'' is the object being tracked. In (\textbf  {c}), the network from \cite  {huang_speed/accuracy_2016} predicts bounding boxes for each gyro, using $16\times 16$ boxes centered on each gyro's ground truth position for training. Although each box bounds its dot, it does not center itself around the dot, preventing location inference. The network is resilient to a translational shift (\textbf  {d}) of $60$ pixels. Note the lower confidence scores, with one gyro receiving $< 50\%$.\relax }}{2}{figure.caption.2}}
\newlabel{fig:gyro-image}{{2}{2}{In (\textbf {a}), the full gyroscopic model for topological metamaterials from \cite {nash_topological_2015}, where each individual gyro (\textbf {b}) is constrained to motion inside its circle. The center ``dot'' is the object being tracked. In (\textbf {c}), the network from \cite {huang_speed/accuracy_2016} predicts bounding boxes for each gyro, using $16\times 16$ boxes centered on each gyro's ground truth position for training. Although each box bounds its dot, it does not center itself around the dot, preventing location inference. The network is resilient to a translational shift (\textbf {d}) of $60$ pixels. Note the lower confidence scores, with one gyro receiving $< 50\%$.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Experiment}{2}{section.2}}
\newlabel{sec:experiment}{{2}{2}{Experiment}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}First Approach}{2}{subsection.2.1}}
\newlabel{sec:first-approach}{{2.1}{2}{First Approach}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Regression using Convolutional Neural Networks on Small Thumbnails}{2}{section.3}}
\newlabel{sec:conv-regr}{{3}{2}{Regression using Convolutional Neural Networks on Small Thumbnails}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Design}{2}{subsection.3.1}}
\newlabel{sec:design}{{3.1}{2}{Design}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Datasets}{2}{subsection.3.2}}
\newlabel{sec:datasets}{{3.2}{2}{Datasets}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Results}{2}{subsection.3.3}}
\newlabel{sec:results}{{3.3}{2}{Results}{subsection.3.3}{}}
\citation{nash_topological_2015}
\citation{nash_topological_2015}
\newlabel{fig:thumbnail_000_12x12_centered_images/centered_test_4}{{3a}{3}{\relax }{figure.caption.3}{}}
\newlabel{sub@fig:thumbnail_000_12x12_centered_images/centered_test_4}{{a}{3}{\relax }{figure.caption.3}{}}
\newlabel{thumbnail_000_12x12_centered_images/rand_bg_normal_test_3}{{3b}{3}{\relax }{figure.caption.3}{}}
\newlabel{sub@thumbnail_000_12x12_centered_images/rand_bg_normal_test_3}{{b}{3}{\relax }{figure.caption.3}{}}
\newlabel{thumbnail_000_30x30_rand_bg_normal_images/rand_bg_normal_test_4}{{3c}{3}{\relax }{figure.caption.3}{}}
\newlabel{sub@thumbnail_000_30x30_rand_bg_normal_images/rand_bg_normal_test_4}{{c}{3}{\relax }{figure.caption.3}{}}
\newlabel{thumbnail_000_30x30_rand_bg_normal_images/centered_test_4}{{3d}{3}{\relax }{figure.caption.3}{}}
\newlabel{sub@thumbnail_000_30x30_rand_bg_normal_images/centered_test_4}{{d}{3}{\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Isolated gyro images from the test set. Ground truth predictions are marked in green, inferred positions in red. The network trained on the $12\times 12$ \emph  {centered} dataset (\textbf  {a}) achieves sub-pixel accuracy relative to ground truths on the centered test set. The same network performs poorly (\textbf  {b}) when tested on randomly offset thumbnails with random noise background. The $30\times 30$ dataset with random perturbations and background noise (\textbf  {c}) produces a resilient network that performs well on centered data, after learning on the perturbed set (\textbf  {d}).\relax }}{3}{figure.caption.3}}
\newlabel{fig:thumbnails}{{3}{3}{Isolated gyro images from the test set. Ground truth predictions are marked in green, inferred positions in red. The network trained on the $12\times 12$ \emph {centered} dataset (\textbf {a}) achieves sub-pixel accuracy relative to ground truths on the centered test set. The same network performs poorly (\textbf {b}) when tested on randomly offset thumbnails with random noise background. The $30\times 30$ dataset with random perturbations and background noise (\textbf {c}) produces a resilient network that performs well on centered data, after learning on the perturbed set (\textbf {d}).\relax }{figure.caption.3}{}}
\newlabel{thumbnail_000_12x12_rand_bg_normal_images/rand_bg_normal_test_3}{{4a}{3}{\relax }{figure.caption.4}{}}
\newlabel{sub@thumbnail_000_12x12_rand_bg_normal_images/rand_bg_normal_test_3}{{a}{3}{\relax }{figure.caption.4}{}}
\newlabel{thumbnail_000_12x12_rand_bg_normal_images/dot_001_rand_bg_normal_test_3}{{4b}{3}{\relax }{figure.caption.4}{}}
\newlabel{sub@thumbnail_000_12x12_rand_bg_normal_images/dot_001_rand_bg_normal_test_3}{{b}{3}{\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Test images from the perturbed dataset (\textbf  {a}) show network performance for the gyro on which it was trained. That same network was tested on a perturbed test set from \emph  {a different gyro} (\textbf  {b}).\relax }}{3}{figure.caption.4}}
\newlabel{fig:different-dot}{{4}{3}{Test images from the perturbed dataset (\textbf {a}) show network performance for the gyro on which it was trained. That same network was tested on a perturbed test set from \emph {a different gyro} (\textbf {b}).\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Ground Truth Precision}{3}{subsection.3.4}}
\newlabel{sec:ground-truth-prec}{{3.4}{3}{Ground Truth Precision}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Artificial Dataset Generation}{3}{section.4}}
\newlabel{sec:artif-datas-gener}{{4}{3}{Artificial Dataset Generation}{section.4}{}}
\citation{nash_topological_2015}
\citation{huang_speed/accuracy_2016}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance for various networks on individual cropped datasets, as in Figures \ref  {fig:thumbnails} and \ref  {fig:different-dot}. Networks trained on the perturbed dataset achieved sub-pixel accuracy.\relax }}{4}{table.caption.5}}
\newlabel{tab:network-performance}{{1}{4}{Performance for various networks on individual cropped datasets, as in Figures \ref {fig:thumbnails} and \ref {fig:different-dot}. Networks trained on the perturbed dataset achieved sub-pixel accuracy.\relax }{table.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A directed graph modeling dependencies in the experiment. We propose that the source nodes in such a graph determine which qualities should be perturbed in an artificial dataset.\relax }}{4}{figure.caption.6}}
\newlabel{fig:dependencies}{{5}{4}{A directed graph modeling dependencies in the experiment. We propose that the source nodes in such a graph determine which qualities should be perturbed in an artificial dataset.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{4}{section.5}}
\newlabel{sec:conclusions}{{5}{4}{Conclusions}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Future Work}{4}{subsection.5.1}}
\newlabel{sec:future-work}{{5.1}{4}{Future Work}{subsection.5.1}{}}
\bibstyle{asmems4}
\bibdata{report}
\bibcite{parkhi_cats_2012}{1}
\bibcite{huang_speed/accuracy_2016}{2}
\bibcite{martin_abadi_tensorflow:_2015}{3}
\bibcite{nash_topological_2015}{4}
\bibcite{krizhevsky_imagenet_2012}{5}
