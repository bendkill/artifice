\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{eq:1}{{1}{1}{}{equation.0.1}{}}
\newlabel{fig:object-positions-in-label-space}{{1a}{2}{Subfigure 1a}{subfigure.1.1}{}}
\newlabel{sub@fig:object-positions-in-label-space}{{(a)}{a}{Subfigure 1a\relax }{subfigure.1.1}{}}
\newlabel{fig:known-label-space-boundaries}{{1b}{2}{Subfigure 1b}{subfigure.1.2}{}}
\newlabel{sub@fig:known-label-space-boundaries}{{(b)}{b}{Subfigure 1b\relax }{subfigure.1.2}{}}
\newlabel{fig:augmentation-without-resampling}{{1c}{2}{Subfigure 1c}{subfigure.1.3}{}}
\newlabel{sub@fig:augmentation-without-resampling}{{(c)}{c}{Subfigure 1c\relax }{subfigure.1.3}{}}
\newlabel{fig:resampling-from-known-label-space-boundary}{{1d}{2}{Subfigure 1d}{subfigure.1.4}{}}
\newlabel{sub@fig:resampling-from-known-label-space-boundary}{{(d)}{d}{Subfigure 1d\relax }{subfigure.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces For any experiment, object positions in label-space will have some natural distribution evident in the original dataset, such as in (\ref  {fig:object-positions-in-label-space}). Generally, an experimenter knows absolute boundaries for each object's distribution, such as in (\ref  {fig:known-label-space-boundaries}), because she imposes them during collection. To analyze object positions (or other properties) in an unbiased way, a Deep Neural Network (DNN) should be trained on images corresponding to labels from within that boundary. (\ref  {fig:augmentation-without-resampling}) shows how one might select new points in label-space to generate images for, keeping all the original points. (\ref  {fig:resampling-from-known-label-space-boundary}) shows a resampling that keeps original points only within the desired distribution, preventing dataset bias.}}{2}{figure.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{2}{subfigure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{2}{subfigure.1.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{2}{subfigure.1.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{2}{subfigure.1.4}}
\newlabel{fig:label-space}{{1}{2}{For any experiment, object positions in label-space will have some natural distribution evident in the original dataset, such as in (\ref {fig:object-positions-in-label-space}). Generally, an experimenter knows absolute boundaries for each object's distribution, such as in (\ref {fig:known-label-space-boundaries}), because she imposes them during collection. To analyze object positions (or other properties) in an unbiased way, a Deep Neural Network (DNN) should be trained on images corresponding to labels from within that boundary. (\ref {fig:augmentation-without-resampling}) shows how one might select new points in label-space to generate images for, keeping all the original points. (\ref {fig:resampling-from-known-label-space-boundary}) shows a resampling that keeps original points only within the desired distribution, preventing dataset bias}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Given original images observed during experiment time $X \in \mathbb  {R}^{h\times w}$, a DNN-facilitated method for image analysis aims to minimize the analysis provided by an experimenter. Therefore, we select a query set $X_Q$ of images that will most facilitate learning. For each of these images, the experimenter provides analysis which consists of labels $y_i$ for each image (shown in red). In order to create training images $X_T$, we add new images by transforming elements of $X_Q$. For instance, we might translate image objects corresponding with new points in label-space, obtained as in Fig. (\ref  {fig:label-space}). Training annotations $A_T$ are obtained from $Y_T$ according to Eq. (\ref  {eq:1}), after which we train the object detection DNN on $(X_T, A_T)$.}}{3}{figure.2}}
\newlabel{fig:training}{{2}{3}{Given original images observed during experiment time $X \in \mathbb {R}^{h\times w}$, a DNN-facilitated method for image analysis aims to minimize the analysis provided by an experimenter. Therefore, we select a query set $X_Q$ of images that will most facilitate learning. For each of these images, the experimenter provides analysis which consists of labels $y_i$ for each image (shown in red). In order to create training images $X_T$, we add new images by transforming elements of $X_Q$. For instance, we might translate image objects corresponding with new points in label-space, obtained as in Fig. (\ref {fig:label-space}). Training annotations $A_T$ are obtained from $Y_T$ according to Eq. (\ref {eq:1}), after which we train the object detection DNN on $(X_T, A_T)$}{figure.2}{}}
